{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPy5M0W34j0aMEAdMUvrRVk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries Imports"
      ],
      "metadata": {
        "id": "oxvcqN9KSC1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "bGkDB9b6QIbB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "ARDlHmrqSLU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "apgATKE9QBPr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "def evaluate_model_with_outputs(model, inputs, outputs, batch_size, evaluation_type:bool):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      model: The PyTorch model to be evaluated.\n",
        "      inputs: List of input tensors.\n",
        "      outputs: List of model output tensors.\n",
        "      evaluation_type: 0 in case of training, 1 in testing\n",
        "\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    correct_predictions = 0\n",
        "    if (evaluation_type == 0):\n",
        "      total_samples = (len(inputs)-1)*batch_size+len(inputs[len(inputs)-1])\n",
        "    else:\n",
        "      total_samples=len(inputs)\n",
        "\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation during evaluation\n",
        "        for input_tensor, output_tensor in zip(inputs, outputs):\n",
        "\n",
        "            predictions = model(input_tensor)\n",
        "            # torch.max returns a tuple of 2 elements\n",
        "            # The first tuple is the value of the max between the output & 1\n",
        "            # Which is intentionally ignored\n",
        "            # The second tuple is the index of the max value\n",
        "            _, predicted_classes = torch.max(predictions, 1)\n",
        "            correct_predictions += (predicted_classes == output_tensor).sum().item()\n",
        "\n",
        "    accuracy = correct_predictions / total_samples\n",
        "\n",
        "    print(f\"{evaluation_type} Model Accuracy: {accuracy * 100:.2f}%\")"
      ]
    }
  ]
}
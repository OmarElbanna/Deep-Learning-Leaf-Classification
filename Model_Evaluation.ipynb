{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLiQ4RUP3jKYIh55i/cpi4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries Imports"
      ],
      "metadata": {
        "id": "oxvcqN9KSC1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "bGkDB9b6QIbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "ARDlHmrqSLU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apgATKE9QBPr"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_with_outputs(model, inputs, outputs, batch_size, evaluation_type:bool):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      model: The PyTorch model to be evaluated.\n",
        "      inputs: List of input tensors.\n",
        "      outputs: List of model output tensors.\n",
        "      evaluation_type: 0 in case of training, 1 in testing\n",
        "\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    correct_predictions = 0\n",
        "    samples_in_epoch = 0\n",
        "    correct_predictions_in_epochs = 0\n",
        "    accuracy_in_epoch = 0\n",
        "    accuracy_list =[]\n",
        "    num_epochs = 0\n",
        "\n",
        "    if (evaluation_type == 0):\n",
        "      total_samples = (len(inputs)-1)*batch_size+len(inputs[len(inputs)-1])\n",
        "\n",
        "      with torch.no_grad():  # Disable gradient calculation during evaluation\n",
        "        for input_tensor, output_tensor in zip(inputs, outputs):\n",
        "\n",
        "          num_epochs+=1\n",
        "          samples_in_epoch = len(input_tensor)\n",
        "\n",
        "          predictions = model(input_tensor)\n",
        "          _, predicted_classes = torch.max(predictions, 1)\n",
        "\n",
        "          correct_predictions_in_epochs = (predicted_classes == output_tensor).sum().item()\n",
        "          accuracy_in_epoch = correct_predictions_in_epochs/samples_in_epoch\n",
        "          accuracy_list.append(accuracy_in_epoch)\n",
        "\n",
        "          correct_predictions += correct_predictions_in_epochs\n",
        "\n",
        "      # Plot the accuracy over epochs\n",
        "      plt.plot(range(1, num_epochs+1), accuracy_list, marker='o')\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.ylabel('Accuracy')\n",
        "      plt.title('Accuracy vs Epoch')\n",
        "      plt.grid(True)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "    else:\n",
        "      total_samples=len(inputs)\n",
        "\n",
        "      with torch.no_grad():  # Disable gradient calculation during evaluation\n",
        "          # Forward pass\n",
        "          predictions = model(inputs)\n",
        "\n",
        "          # Assuming outputs contain ground truth labels\n",
        "          _, predicted_classes = torch.max(predictions, 1)\n",
        "          correct_predictions = (predicted_classes == outputs).sum().item()\n",
        "\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    print(f\"{evaluation_type} Model Accuracy: {accuracy * 100:.2f}%\")"
      ]
    }
  ]
}